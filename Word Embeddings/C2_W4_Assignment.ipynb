{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bc5b11",
   "metadata": {},
   "source": [
    "# Assignment 4: Word Embeddings \n",
    "\n",
    "Welcome to the fourth (and last) programming assignment of Course 2! \n",
    "\n",
    "In this assignment, you will practice how to compute word embeddings and use them for sentiment analysis.\n",
    "- To implement sentiment analysis, you can go beyond counting the number of positive words and negative words. \n",
    "- You can find a way to represent each word numerically, by a vector. \n",
    "- The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures. \n",
    "\n",
    "In this assignment, you will explore a classic way of generating word embeddings or representations.\n",
    "- You will implement a famous model called the continuous bag of words (CBOW) model. \n",
    "\n",
    "By completing this assignment you will:\n",
    "\n",
    "- Train word vectors from scratch.\n",
    "- Learn how to create batches of data.\n",
    "- Understand how backpropagation works.\n",
    "- Plot and visualize your learned word vectors.\n",
    "\n",
    "Knowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing.\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/saGQf/how-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04d698",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [1 The Continuous bag of words model](#1)\n",
    "- [2 Training the Model](#2)\n",
    "    - [2.0 Initialize the model](#2)\n",
    "        - [Exercise 01](#ex-01)\n",
    "    - [2.1 Softmax Function](#2.1)\n",
    "        - [Exercise 02](#ex-02)\n",
    "    - [2.2 Forward Propagation](#2.2)\n",
    "        - [Exercise 03](#ex-03)\n",
    "    - [2.3 Cost Function](#2.3)\n",
    "    - [2.4 Backproagation](#2.4)\n",
    "        - [Exercise 04](#ex-04)\n",
    "    - [2.5 Gradient Descent](#2.5)\n",
    "        - [Exercise 05](#ex-05)\n",
    "- [3 Visualizing the word vectors](#3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7bc5c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1. The Continuous bag of words model\n",
    "\n",
    "Let's take a look at the following sentence: \n",
    ">**'I am happy because I am learning'**. \n",
    "\n",
    "- In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\n",
    "- For example, if you were to choose a context half-size of say $C = 2$, then you would try to predict the word **happy** given the context that includes 2 words before and 2 words after the center word:\n",
    "\n",
    "> $C$ words before: [I, am] \n",
    "\n",
    "> $C$ words after: [because, I] \n",
    "\n",
    "- In other words:\n",
    "\n",
    "$$context = [I,am, because, I]$$\n",
    "$$target = happy$$\n",
    "\n",
    "The structure of your model will look like this:\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/word2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 1 </div>\n",
    "\n",
    "Where $\\bar x$ is the average of all the one hot vectors of the context words. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/mean_vec2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 2 </div>\n",
    "\n",
    "Once you have encoded all the context words, you can use $\\bar x$ as the input to your model. \n",
    "\n",
    "The architecture you will be implementing is as follows:\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " a &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
    " \\hat y &= softmax(z)   \\tag{4} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88afe16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Python libraries and helper functions (in utils2) \n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# from utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
    "# import w4_unittest\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
    "import w4_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76918f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jay/nltk_data', '/home/jay/anaconda3/nltk_data', '/home/jay/anaconda3/share/nltk_data', '/home/jay/anaconda3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Download sentence tokenizer\n",
    "# nltk.data.path.append('.')\n",
    "nltk.data.path.append('.')\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f5ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 60976 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
     ]
    }
   ],
   "source": [
    "# # Load, tokenize and process the data\n",
    "# import re                                                           #  Load the Regex-modul\n",
    "# with open('./data/shakespeare.txt') as f:\n",
    "#     data = f.read()                                                 #  Read in the data\n",
    "# data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
    "# data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
    "# data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
    "# print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample\n",
    "\n",
    "\n",
    "import re\n",
    "with open('./data/shakespeare.txt') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = re.sub(r'[,!?;-]', '.', data)\n",
    "data = nltk.word_tokenize(data)\n",
    "data = [ch.lower() for ch in data\n",
    "       if ch.isalpha()\n",
    "       or ch == '.']\n",
    "print('Number of tokens:', len(data), '\\n', data[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007c4f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
     ]
    }
   ],
   "source": [
    "# # Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "# fdist = nltk.FreqDist(word for word in data)\n",
    "# print(\"Size of vocabulary: \",len(fdist) )\n",
    "# print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq.\n",
    "\n",
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(data)\n",
    "\n",
    "print(\"Size of vocabulary: \", len(fdist))\n",
    "print(\"Most frequent tokens: \", fdist.most_common(20)) # prints the 20 most frequent words and their freq.\n",
    "\n",
    "# or,\n",
    "# countr = Counter(data)\n",
    "# countr.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbea87",
   "metadata": {},
   "source": [
    "#### Mapping words to indices and indices to words\n",
    "We provide a helper function to create a dictionary that maps words to indices and indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea8f747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n"
     ]
    }
   ],
   "source": [
    "# # get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "# word2Ind, Ind2word = get_dict(data)\n",
    "# V = len(word2Ind)\n",
    "# print(\"Size of vocabulary: \", V)\n",
    "\n",
    "# Get dict creates two dictionaries, converting words to indices and viceversa\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692c8d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'king' :   2744\n",
      "Word which has index 2743:   kinds\n"
     ]
    }
   ],
   "source": [
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \",Ind2word[2743] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f41a00d-1544-4cb4-9f5b-1b989520ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp_get_batches(data, word2Ind, V, C, batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    i = 0\n",
    "    for x, y in tmp_get_vectors(data, word2Ind, V, C):\n",
    "        if len(batch_x) < batch_size:\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "        else:\n",
    "            yield np.array(batch_x).T, np.array(batch_y).T # need (V, batch_size) so transposing\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "    \n",
    "def tmp_get_one_hot_vector(center_word, word2Ind):\n",
    "    one_hot_vector = np.zeros(len(word2Ind))\n",
    "    one_hot_vector[word2Ind[center_word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def tmp_get_vector_from_context_words(context_word_list, word2Ind):\n",
    "    one_hot_vectors = [tmp_get_one_hot_vector(word, word2Ind) for word in context_word_list]\n",
    "    return np.mean(one_hot_vectors, axis=0)\n",
    "\n",
    "def tmp_get_vectors(data, word2Ind, V, C):\n",
    "    i = C\n",
    "    while True:\n",
    "        center_word = data[i]\n",
    "        context_word_list = data[i-C:i] + data[(i+1):(i+C+1)]\n",
    "        \n",
    "        y = tmp_get_one_hot_vector(center_word, word2Ind)\n",
    "        x = tmp_get_vector_from_context_words(context_word_list, word2Ind)\n",
    "        \n",
    "        yield x, y\n",
    "        i += 1\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5104e0",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2 Training the Model\n",
    "\n",
    "### Initializing the model\n",
    "\n",
    "You will now initialize two matrices and two vectors. \n",
    "- The first matrix ($W_1$) is of dimension $N \\times V$, where $V$ is the number of words in your vocabulary and $N$ is the dimension of your word vector.\n",
    "- The second matrix ($W_2$) is of dimension $V \\times N$. \n",
    "- Vector $b_1$ has dimensions $N\\times 1$\n",
    "- Vector $b_2$ has dimensions  $V\\times 1$. \n",
    "- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n",
    "\n",
    "The overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n",
    "\n",
    "<a name='ex-01'></a>\n",
    "### Exercise 01\n",
    "Please use [numpy.random.rand](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.\n",
    "\n",
    "**Note:** In the next cell you will encounter a random seed. Please **DO NOT** modify this seed so your solution can be tested correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7552b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# # UNQ_C1 GRADED FUNCTION: initialize_model\n",
    "# def initialize_model(N,V, random_seed=1):\n",
    "#     '''\n",
    "#     Inputs: \n",
    "#         N:  dimension of hidden vector \n",
    "#         V:  dimension of vocabulary\n",
    "#         random_seed: random seed for consistent results in the unit tests\n",
    "#      Outputs: \n",
    "#         W1, W2, b1, b2: initialized weights and biases\n",
    "#     '''\n",
    "    \n",
    "#     ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "#     np.random.seed(random_seed)\n",
    "#     # W1 has shape (N,V)\n",
    "#     W1 = None\n",
    "    \n",
    "#     # W2 has shape (V,N)\n",
    "#     W2 = None\n",
    "    \n",
    "#     # b1 has shape (N,1)\n",
    "#     b1 = None\n",
    "    \n",
    "#     # b2 has shape (V,1)\n",
    "#     b2 = None\n",
    "    \n",
    "#     ### END CODE HERE ###\n",
    "#     return W1, W2, b1, b2\n",
    "\n",
    "def initialize_model(N, V, random_seed=1):\n",
    "    np.random.seed(random_seed)\n",
    "    W1 = np.random.rand(N, V)\n",
    "    \n",
    "    W2 = np.random.rand(V, N)\n",
    "    \n",
    "    b1 = np.random.rand(N, 1)\n",
    "    \n",
    "    b2 = np.random.rand(V, 1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eecb29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_W1.shape: (4, 10)\n",
      "tmp_W2.shape: (10, 4)\n",
      "tmp_b1.shape: (4, 1)\n",
      "tmp_b2.shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# # Test your function example.\n",
    "# tmp_N = 4\n",
    "# tmp_V = 10\n",
    "# tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "# assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
    "# assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
    "# print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
    "# print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
    "# print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
    "# print(f\"tmp_b2.shape: {tmp_b2.shape}\")\n",
    "\n",
    "# Test your function example.\n",
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N, tmp_V)\n",
    "\n",
    "assert tmp_W1.shape == (tmp_N, tmp_V)\n",
    "assert tmp_W2.shape == (tmp_V, tmp_N)\n",
    "\n",
    "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dab033",
   "metadata": {},
   "source": [
    "##### Expected Output \n",
    "\n",
    "```CPP\n",
    "tmp_W1.shape: (4, 10)\n",
    "tmp_W2.shape: (10, 4)\n",
    "tmp_b1.shape: (4, 1)\n",
    "tmp_b2.shape: (10, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963a456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_initialize_model(initialize_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac595b53",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 Softmax\n",
    "Before we can start training the model, we need to implement the softmax function as defined in equation 5:  \n",
    "\n",
    "<br>\n",
    "$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} $$\n",
    "\n",
    "- Array indexing in code starts at 0.\n",
    "- $V$ is the number of words in the vocabulary (which is also the number of rows of $z$).\n",
    "- $i$ goes from 0 to |V| - 1.\n",
    "\n",
    "\n",
    "<a name='ex-02'></a>\n",
    "### Exercise 02\n",
    "**Instructions**: Implement the softmax function below. \n",
    "\n",
    "- Assume that the input $z$ to `softmax` is a 2D array\n",
    "- Each training example is represented by a vector of shape (V, 1) in this 2D array.\n",
    "- There may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency.  Let's call the batch size lowercase $m$, so the $z$ array has shape (V, m)\n",
    "- When taking the sum from $i=1 \\cdots V-1$, take the sum for each column (each example) separately.\n",
    "\n",
    "Please use\n",
    "- numpy.exp\n",
    "- numpy.sum (set the axis so that you take the sum of each column in z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9511e316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# # UNQ_C2 GRADED FUNCTION: softmax\n",
    "# def softmax(z):\n",
    "#     '''\n",
    "#     Inputs: \n",
    "#         z: output scores from the hidden layer\n",
    "#     Outputs: \n",
    "#         yhat: prediction (estimate of y)\n",
    "#     '''\n",
    "#     ### START CODE HERE (Replace instances of 'None' with your own code) ###\n",
    "#     # Calculate yhat (softmax)\n",
    "#     yhat = None\n",
    "#     ### END CODE HERE ###\n",
    "#     return yhat\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z, axis=0)\n",
    "    return e_z / sum_e_z\n",
    "\n",
    "def relu(z):\n",
    "    temp = z.copy()\n",
    "    temp[temp<0] = 0\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beea7116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.73105858, 0.88079708],\n",
       "       [0.5       , 0.26894142, 0.11920292]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp = np.array([[1,2,3],\n",
    "                [1,1,1]\n",
    "               ])\n",
    "tmp_sm = softmax(tmp)\n",
    "display(tmp_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582c843",
   "metadata": {},
   "source": [
    "##### Expected Ouput\n",
    "\n",
    "```CPP\n",
    "array([[0.5       , 0.73105858, 0.88079708],\n",
    "       [0.5       , 0.26894142, 0.11920292]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725d293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_softmax(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65884622",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 Forward propagation\n",
    "\n",
    "<a name='ex-03'></a>\n",
    "### Exercise 03\n",
    "Implement the forward propagation $z$ according to equations (1) to (3). <br>\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " a &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For that, you will use as activation the Rectified Linear Unit (ReLU) given by:\n",
    "\n",
    "$$f(h)=\\max (0,h) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f36de",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use numpy.maximum(x1,x2) to get the maximum of two values</li>\n",
    "    <li>Use numpy.dot(A,B) to matrix multiply A and B</li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d001a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# # UNQ_C3 GRADED FUNCTION: forward_prop\n",
    "# def forward_prop(x, W1, W2, b1, b2):\n",
    "#     '''\n",
    "#     Inputs: \n",
    "#         x:  average one hot vector for the context \n",
    "#         W1, W2, b1, b2:  matrices and biases to be learned\n",
    "#      Outputs: \n",
    "#         z:  output score vector\n",
    "#     '''\n",
    "    \n",
    "#     ### START CODE HERE (Replace instances of 'None' with your own code) ###\n",
    "#     # Calculate h\n",
    "#     h = None\n",
    "  \n",
    "#     # Apply the relu on h, \n",
    "#     # store the relu in h\n",
    "#     h = None\n",
    "\n",
    "#     # Calculate z\n",
    "#     z = None\n",
    "\n",
    "#     ### END CODE HERE ###\n",
    "\n",
    "#     return z, h\n",
    "\n",
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    # Calculating z1 and h1\n",
    "    z1 = np.dot(W1, x) + b1 # (NxV).(Vx1) + (Nx1) => (Nx1) or, (NxV).(VxM) + (Nx1) => (N, M) with broadcasting rule of numpy\n",
    "    h1 = relu(np.dot(W1, x) + b1)\n",
    "    \n",
    "    # Calculate z2 and h2\n",
    "    z2 = np.dot(W2, h1) + b2 # (VxN).(Nx1) + (Vx1) => (Vx1) or, (VxN).(Nx1) + (Vx1) => (Vx1)\n",
    "    h2 = softmax(z2)\n",
    "    \n",
    "    return h1, h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c791c505-103a-4a8f-9236-7615840f8d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 5],\n",
       "       [2, 6],\n",
       "       [5, 7]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[2, 4], [1, 5], [4, 6]]) + np.array([[1, 1], [1, 1], [1, 1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b36840-1a1a-4186-acb2-2864290a2d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "x has shape (3, 1)\n",
      "N is 2 and vocabulary size V is 3\n",
      "h has shape (2, 1)\n",
      "h has values: \n",
      " [[0.92477674]\n",
      " [1.02487333]]\n"
     ]
    }
   ],
   "source": [
    "#### # Test the function\n",
    "\n",
    "# # Create some inputs\n",
    "# tmp_N = 2\n",
    "# tmp_V = 3\n",
    "# tmp_x = np.array([[0,1,0]]).T\n",
    "# #print(tmp_x)\n",
    "# tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
    "\n",
    "# print(f\"x has shape {tmp_x.shape}\")\n",
    "# print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
    "\n",
    "# # call function\n",
    "# tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "# print(\"call forward_prop\")\n",
    "# print()\n",
    "# # Look at output\n",
    "# print(f\"z has shape {tmp_z.shape}\")\n",
    "# print(\"z has values:\")\n",
    "# print(tmp_z)\n",
    "\n",
    "# print()\n",
    "\n",
    "# print(f\"h has shape {tmp_h.shape}\")\n",
    "# print(\"h has values:\")\n",
    "# print(tmp_h)\n",
    "\n",
    "# Test the function\n",
    "\n",
    "# Create some inputs\n",
    "tmp_N = 2\n",
    "tmp_V = 3\n",
    "tmp_x = np.array([[0,1,0]]).T\n",
    "print(tmp_x)\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N, V=tmp_V, random_seed=1)\n",
    "\n",
    "print(f\"x has shape {tmp_x.shape}\")\n",
    "print(f'N is {tmp_N} and vocabulary size V is {tmp_V}')\n",
    "\n",
    "tmp_h1, tmp_h2 = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "# print(\"call forward_prop\")\n",
    "# print()\n",
    "# print(f\"z has shape {tmp_z.shape}\")\n",
    "# print(\"z has values\")\n",
    "# print(tmp_z)\n",
    "# print()\n",
    "\n",
    "print(f\"h has shape {tmp_h1.shape}\")\n",
    "print(\"h has values:\", '\\n', tmp_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85977693",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "```CPP\n",
    "x has shape (3, 1)\n",
    "N is 2 and vocabulary size V is 3\n",
    "call forward_prop\n",
    "\n",
    "z has shape (3, 1)\n",
    "z has values:\n",
    "[[0.55379268]\n",
    " [1.58960774]\n",
    " [1.50722933]]\n",
    "\n",
    "h has shape (2, 1)\n",
    "h has values:\n",
    "[[0.92477674]\n",
    " [1.02487333]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e589d02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong output values for z vector.\n",
      "\t Expected: [[0.55379268]\n",
      " [1.58960774]\n",
      " [1.50722933]] \n",
      "\tGot: [[0.92477674]\n",
      " [1.02487333]].\n",
      "Wrong type for h vector.\n",
      "\t Expected: [[0.92477674]\n",
      " [1.02487333]] \n",
      "\tGot: [[0.15595727]\n",
      " [0.43939427]\n",
      " [0.40464846]].\n",
      "Wrong output shape for z vector.\n",
      "\t Expected: (3, 1) \n",
      "\tGot: (2, 1).\n",
      "Wrong output shape for h vector.\n",
      "\t Expected: (2, 1) \n",
      "\tGot: (3, 1).\n",
      "\u001b[92m 2  Tests passed\n",
      "\u001b[91m 4  Tests failed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_forward_prop(forward_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e47e2",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "## 2.3 Cost function\n",
    "\n",
    "- We have implemented the *cross-entropy* cost function for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba6ae54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # compute_cost: cross-entropy cost function\n",
    "# def compute_cost(y, yhat, batch_size):\n",
    "\n",
    "#     # cost function \n",
    "#     logprobs = np.multiply(np.log(yhat),y)\n",
    "#     cost = - 1/batch_size * np.sum(logprobs)\n",
    "#     cost = np.squeeze(cost)\n",
    "#     return cost\n",
    "\n",
    "# compute_cost: cross-entropy cost function\n",
    "\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "    temp = np.multiply(y, np.log(yhat))\n",
    "    cost = - (1/batch_size) * np.sum(temp)\n",
    "    return np.squeeze(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c120568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_V: 5775\n",
      "tmp_x.shape (5775, 4)\n",
      "tmp_y.shape (5775, 4)\n",
      "tmp_W1.shape (50, 5775)\n",
      "tmp_W2.shape (5775, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (5775, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.35113937735471"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Test the function\n",
    "# tmp_C = 2\n",
    "# tmp_N = 50\n",
    "# tmp_batch_size = 4\n",
    "# tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "# tmp_V = len(word2Ind)\n",
    "\n",
    "# tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "        \n",
    "# print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "# print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "# tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "# print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "# print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "# print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "# print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "# tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "# print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "# print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "# tmp_yhat = softmax(tmp_z)\n",
    "# print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "# tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
    "# print(\"call compute_cost\")\n",
    "# print(f\"tmp_cost {tmp_cost:.4f}\")\n",
    "  \n",
    "\n",
    "\n",
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "\n",
    "tmp_V = len(tmp_word2Ind)\n",
    "print(f\"tmp_V: {tmp_V}\")\n",
    "batch_training_examples = tmp_get_batches(data, tmp_word2Ind, tmp_V, tmp_C, tmp_batch_size)\n",
    "tmp_x, tmp_y = next(batch_training_examples)\n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N, tmp_V)\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "tmp_h1, tmp_yhat = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "\n",
    "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
    "tmp_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d364b",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "\n",
    "```CPP\n",
    "tmp_x.shape (5778, 4)\n",
    "tmp_y.shape (5778, 4)\n",
    "tmp_W1.shape (50, 5778)\n",
    "tmp_W2.shape (5778, 50)\n",
    "tmp_b1.shape (50, 1)\n",
    "tmp_b2.shape (5778, 1)\n",
    "tmp_z.shape: (5778, 4)\n",
    "tmp_h.shape: (50, 4)\n",
    "tmp_yhat.shape: (5778, 4)\n",
    "call compute_cost\n",
    "tmp_cost 8.9542\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8f8e4",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "## 2.4 Training the Model - Backpropagation\n",
    "\n",
    "<a name='ex-04'></a>\n",
    "### Exercise 04\n",
    "Now that you have understood how the CBOW model works, you will train it. <br>\n",
    "You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d28f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# # UNQ_C4 GRADED FUNCTION: back_prop\n",
    "# def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "#     '''\n",
    "#     Inputs: \n",
    "#         x:  average one hot vector for the context \n",
    "#         yhat: prediction (estimate of y)\n",
    "#         y:  target vector\n",
    "#         h:  hidden vector (see eq. 1)\n",
    "#         W1, W2, b1, b2:  matrices and biases  \n",
    "#         batch_size: batch size \n",
    "#      Outputs: \n",
    "#         grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "#     '''\n",
    "#     ### START CODE HERE (Replace instanes of 'None' with your code) ###\n",
    "#     # Compute l1 as W2^T (Yhat - Y)\n",
    "#     # and re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n",
    "#     l1 = None\n",
    "\n",
    "#     # Apply relu to l1\n",
    "#     l1 = None\n",
    "\n",
    "#     # compute the gradient for W1\n",
    "#     grad_W1 = None\n",
    "\n",
    "#     # Compute gradient of W2\n",
    "#     grad_W2 = None\n",
    "    \n",
    "#     # compute gradient for b1\n",
    "#     grad_b1 = None\n",
    "\n",
    "#     # compute gradient for b2\n",
    "#     grad_b2 = None\n",
    "#     ### END CODE HERE ####\n",
    "    \n",
    "#     return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "\n",
    "\n",
    "def back_prop(x, yhat, y, h1, W1, W2, b1, b2, batch_size):\n",
    "    l1 = relu(np.dot(W2.T, yhat - y)) # (V, N).T . ((V, M) - (V, M)) => (N, M)\n",
    "    \n",
    "    grad_W1 = (1/batch_size) * np.dot(l1, x.T) # (N, M).((V, M).T) => (N, V)\n",
    "    grad_W2 = (1/batch_size) * np.dot((yhat - y), h1.T)  # ((V, M) - (V, M)).((N, M).T) => (V, N)\n",
    "    grad_b1 = (1/batch_size) * np.sum(l1, axis=1, keepdims=True) # np.sum((N, M), axis=1, keepdims=True) =>(N, 1)\n",
    "    grad_b2 = (1/batch_size) * np.sum(yhat - y, axis=1, keepdims=True) # (V, 1)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a888b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get a batch of data\n",
      "tmp_x.shape (5775, 4)\n",
      "tmp_y.shape (5775, 4)\n",
      "\n",
      "Initialize weights and biases\n",
      "tmp_W1.shape (50, 5775)\n",
      "tmp_W2.shape (5775, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (5775, 1)\n",
      "\n",
      "Forward prop to get h1 and h2\n",
      "tmp_h1.shape: (50, 4)\n",
      "h2.shape: (5775, 4)\n",
      "\n",
      "Calling back_prop\n",
      "\n",
      "tmp_grad_W1.shape: (50, 5775)\n",
      "tmp_grad_W2.shape: (5775, 50)\n",
      "tmp_grad_b1.shape: (50, 1)\n",
      "tmp_grad_b2.shape: (5775, 1)\n"
     ]
    }
   ],
   "source": [
    "# # Test the function\n",
    "# tmp_C = 2\n",
    "# tmp_N = 50\n",
    "# tmp_batch_size = 4\n",
    "# tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "# tmp_V = len(word2Ind)\n",
    "\n",
    "\n",
    "# # get a batch of data\n",
    "# tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "\n",
    "# print(\"get a batch of data\")\n",
    "# print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "# print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "# print()\n",
    "# print(\"Initialize weights and biases\")\n",
    "# tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "# print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "# print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "# print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "# print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "# print()\n",
    "# print(\"Forwad prop to get z and h\")\n",
    "# tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "# print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "# print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "# print()\n",
    "# print(\"Get yhat by calling softmax\")\n",
    "# tmp_yhat = softmax(tmp_z)\n",
    "# print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "# tmp_m = (2*tmp_C)\n",
    "# tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
    "\n",
    "# print()\n",
    "# print(\"call back_prop\")\n",
    "# print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
    "# print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
    "# print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
    "# print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")\n",
    "\n",
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batchsize = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "# Get a batch of data\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V, tmp_C, tmp_batch_size))\n",
    "print(\"Get a batch of data\")\n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")   # (V, batchsize)\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")   # (V, batchsize)\n",
    "print()\n",
    "print(\"Initialize weights and biases\")\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N, tmp_V)\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "print()\n",
    "print(\"Forward prop to get h1 and h2\")\n",
    "tmp_h1, tmp_yhat = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_h1.shape: {tmp_h1.shape}\")\n",
    "print(f\"h2.shape: {tmp_yhat.shape}\")\n",
    "print()\n",
    "print(\"Calling back_prop\") # back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h1, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
    "print()\n",
    "print(f\"tmp_grad_W1.shape: {tmp_grad_W1.shape}\")\n",
    "print(f\"tmp_grad_W2.shape: {tmp_grad_W2.shape}\")\n",
    "print(f\"tmp_grad_b1.shape: {tmp_grad_b1.shape}\")\n",
    "print(f\"tmp_grad_b2.shape: {tmp_grad_b2.shape}\")\n",
    "\n",
    "# tmp_h1, tmp_yhat = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "# tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h1, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91011dc1",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "\n",
    "```CPP\n",
    "get a batch of data\n",
    "tmp_x.shape (5778, 4)\n",
    "tmp_y.shape (5778, 4)\n",
    "\n",
    "Initialize weights and biases\n",
    "tmp_W1.shape (50, 5778)\n",
    "tmp_W2.shape (5778, 50)\n",
    "tmp_b1.shape (50, 1)\n",
    "tmp_b2.shape (5778, 1)\n",
    "\n",
    "Forwad prop to get z and h\n",
    "tmp_z.shape: (5778, 4)\n",
    "tmp_h.shape: (50, 4)\n",
    "\n",
    "Get yhat by calling softmax\n",
    "tmp_yhat.shape: (5778, 4)\n",
    "\n",
    "call back_prop\n",
    "tmp_grad_W1.shape (50, 5778)\n",
    "tmp_grad_W2.shape (5778, 50)\n",
    "tmp_grad_b1.shape (50, 1)\n",
    "tmp_grad_b2.shape (5778, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6166881",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "back_prop() got an unexpected keyword argument 'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-20d930bc5e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test your function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw4_unittest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_back_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/NLP Specialization/Probabilistic Models/Week 4 - Word Embeddings/Assignment/w4_unittest.py\u001b[0m in \u001b[0;36mtest_back_prop\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_case\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m         tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = target(\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: back_prop() got an unexpected keyword argument 'h'"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_back_prop(back_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329dcbeb",
   "metadata": {},
   "source": [
    "<a name='2.5'></a>\n",
    "## Gradient Descent\n",
    "\n",
    "<a name='ex-05'></a>\n",
    "### Exercise 05\n",
    "Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set. \n",
    "\n",
    "**Hint:** For that, you will use `initialize_model` and the `back_prop` functions which you just created (and the `compute_cost` function). You can also use the provided `get_batches` helper function:\n",
    "\n",
    "```for x, y in get_batches(data, word2Ind, V, C, batch_size):```\n",
    "\n",
    "```...```\n",
    "\n",
    "Also: print the cost after each batch is processed (use batch size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ba4d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# # UNQ_C5 GRADED FUNCTION: gradient_descent\n",
    "\n",
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, random_seed=282,\n",
    "                    initialize_model=initialize_model, get_batches=get_batches, forward_prop=forward_prop,\n",
    "                    compute_cost=compute_cost, back_prop=back_prop):\n",
    "    \n",
    "    # Initialize the bias and weights\n",
    "    W1, W2, b1, b2 = initialize_model(N, V, random_seed=random_seed)\n",
    "    \n",
    "    batch_size = 128\n",
    "    C = 2\n",
    "    iters = 0\n",
    "    \n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        # Forward propagation\n",
    "        h1, yhat = forward_prop(x, W1, W2, b1, b2)\n",
    "        \n",
    "        # Computing cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        \n",
    "        if ((iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "        \n",
    "        # Computing gradients\n",
    "        grad_w1, grad_w2, grad_b1, grad_b2 = back_prop(x, yhat, y, h1, W1, W2, b1, b2, batch_size=batch_size)\n",
    "        \n",
    "        # Updating weights and biases with gradient descent\n",
    "        W1 = W1 - grad_w1 * alpha\n",
    "        W2 = W2 - grad_w2 * alpha\n",
    "        b1 = b1 - grad_b1 * alpha\n",
    "        b2 = b2 - grad_b2 * alpha\n",
    "        \n",
    "        iters += 1\n",
    "        if iters == num_iters:\n",
    "            break\n",
    "        \n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "        \n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "# def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, \n",
    "#                      random_seed=282, initialize_model=initialize_model, \n",
    "#                      get_batches=get_batches, forward_prop=forward_prop, \n",
    "#                      softmax=softmax, compute_cost=compute_cost, \n",
    "#                      back_prop=back_prop):\n",
    "    \n",
    "#     '''\n",
    "#     This is the gradient_descent function\n",
    "    \n",
    "#       Inputs: \n",
    "\n",
    "#         data:      text\n",
    "#         word2Ind:  words to Indices\n",
    "#         N:         dimension of hidden vector  \n",
    "#         V:         dimension of vocabulary \n",
    "#         num_iters: number of iterations  \n",
    "#         random_seed: random seed to initialize the model's matrices and vectors\n",
    "#         initialize_model: your implementation of the function to initialize the model\n",
    "#         get_batches: function to get the data in batches\n",
    "#         forward_prop: your implementation of the function to perform forward propagation\n",
    "#         softmax: your implementation of the softmax function\n",
    "#         compute_cost: cost function (Cross entropy)\n",
    "#         back_prop: your implementation of the function to perform backward propagation\n",
    "#      Outputs: \n",
    "#         W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "#     '''\n",
    "#     W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) #W1=(N,V) and W2=(V,N)\n",
    "\n",
    "#     batch_size = 128\n",
    "# #    batch_size = 512\n",
    "#     iters = 0\n",
    "#     C = 2 \n",
    "    \n",
    "#     for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "#         ### START CODE HERE (Replace instances of 'None' with your own code) ###                \n",
    "#         # get z and h\n",
    "#         z, h = None\n",
    "                \n",
    "#         # get yhat\n",
    "#         yhat = None\n",
    "        \n",
    "#         # get cost\n",
    "#         cost = None\n",
    "#         if ( (iters+1) % 10 == 0):\n",
    "#             print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "            \n",
    "#         # get gradients\n",
    "#         grad_W1, grad_W2, grad_b1, grad_b2 = None\n",
    "        \n",
    "#         # update weights and biases\n",
    "#         W1 = None\n",
    "#         W2 = None\n",
    "#         b1 = None     \n",
    "#         b2 = None\n",
    "\n",
    "#         ### END CODE HERE ###\n",
    "#         iters +=1 \n",
    "#         if iters == num_iters: \n",
    "#             break\n",
    "#         if iters % 100 == 0:\n",
    "#             alpha *= 0.66\n",
    "            \n",
    "#     return W1, W2, b1, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d0ccec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient descent\n",
      "iters: 10 cost: 8.538367\n",
      "iters: 20 cost: 4.449100\n",
      "iters: 30 cost: 16.154438\n",
      "iters: 40 cost: 2.372795\n",
      "iters: 50 cost: 10.508012\n",
      "iters: 60 cost: 7.730859\n",
      "iters: 70 cost: 5.893077\n",
      "iters: 80 cost: 9.354901\n",
      "iters: 90 cost: 10.002799\n",
      "iters: 100 cost: 11.484674\n",
      "iters: 110 cost: 4.625150\n",
      "iters: 120 cost: 4.428295\n",
      "iters: 130 cost: 10.306100\n",
      "iters: 140 cost: 6.705970\n",
      "iters: 150 cost: 3.189159\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "# UNIT TEST COMMENT: Each time this cell is run the cost for each iteration changes slightly (the change is less dramatic after some iterations)\n",
    "# to have this into account let's accept an answer as correct if the cost of iter 15 = 41.6 (without caring about decimal points beyond the first decimal)\n",
    "# 41.66, 41.69778, 41.63, etc should all be valid answers.\n",
    "# C = 2\n",
    "# N = 50\n",
    "# word2Ind, Ind2word = get_dict(data)\n",
    "# V = len(word2Ind)\n",
    "# num_iters = 150\n",
    "# print(\"Call gradient_descent\")\n",
    "# W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\n",
    "\n",
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "print(\"Call gradient descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ec264",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "\n",
    "```python\n",
    "iters: 10 cost: 11.714748\n",
    "iters: 20 cost: 3.788280\n",
    "iters: 30 cost: 9.179923\n",
    "iters: 40 cost: 1.747809\n",
    "iters: 50 cost: 8.706968\n",
    "iters: 60 cost: 10.182652\n",
    "iters: 70 cost: 7.258762\n",
    "iters: 80 cost: 10.214489\n",
    "iters: 90 cost: 9.311061\n",
    "iters: 100 cost: 10.103939\n",
    "iters: 110 cost: 5.582018\n",
    "iters: 120 cost: 4.330974\n",
    "iters: 130 cost: 9.436612\n",
    "iters: 140 cost: 6.875775\n",
    "iters: 150 cost: 2.874090\n",
    "```        \n",
    "        \n",
    "Your numbers may differ a bit depending on which version of Python you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "820a6b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name default_check\n",
      "iters: 10 cost: 9.701559\n",
      "Wrong output shape for W1 matrix.\n",
      "\t Expected: (10, 5778) \n",
      "\tGot: (10, 5775).\n",
      "Wrong output shape for W2 matrix.\n",
      "\t Expected: (5778, 10) \n",
      "\tGot: (5775, 10).\n",
      "Wrong output shape for b2 vector.\n",
      "\t Expected: (5778, 1) \n",
      "\tGot: (5775, 1).\n",
      "Wrong output values for W1 matrix.\n",
      "\t Expected: [[0.36409955 0.38892563 0.12117024 ... 0.21485169 0.8417732  0.4013149 ]\n",
      " [0.13575098 0.54877358 0.39651013 ... 0.64207701 0.6203919  0.97065011]\n",
      " [0.03939965 0.3807733  0.41687347 ... 0.18933903 0.40646899 0.71397686]\n",
      " ...\n",
      " [0.21540556 0.19491621 0.18807501 ... 0.39607305 0.78038406 0.6061392 ]\n",
      " [0.46906148 0.14242556 0.21027327 ... 0.87438456 0.75839127 0.97442377]\n",
      " [0.1400866  0.23955149 0.05433805 ... 0.71292201 0.93679829 0.72879085]] \n",
      "\tGot: [[0.35801618 0.38403625 0.12117024 ... 0.17186261 0.25644932 0.30847224]\n",
      " [0.21243742 0.84039803 0.4013149  ... 0.39430508 0.87354257 0.9487626 ]\n",
      " [0.04710886 0.73883011 0.71178276 ... 0.4011204  0.7963532  0.95414673]\n",
      " ...\n",
      " [0.50396525 0.29371189 0.51560631 ... 0.15122536 0.36083922 0.67154991]\n",
      " [0.95257741 0.14621986 0.71631176 ... 0.89558407 0.78072534 0.51184918]\n",
      " [0.27589409 0.03278513 0.16997816 ... 0.81155232 0.81384866 0.02833803]].\n",
      "Wrong output values for W2 matrix.\n",
      "\t Expected: [[1.04159784 0.26177388 0.848308   ... 0.74728741 0.94018686 0.13539754]\n",
      " [0.97860239 0.65316393 0.10570866 ... 1.07529556 0.88030501 0.13926188]\n",
      " [0.74306976 0.69342278 0.77137102 ... 0.46450767 0.16699697 0.19468542]\n",
      " ...\n",
      " [0.61898739 0.52199188 0.44596942 ... 0.91220257 0.84879163 0.93975761]\n",
      " [0.46573742 0.27582959 0.85770989 ... 0.55741427 0.76747582 0.59149174]\n",
      " [0.88165063 0.52037371 0.49309416 ... 0.17477049 0.81721038 0.49459065]] \n",
      "\tGot: [[0.47645144 0.88494448 0.92185354 ... 0.27766707 0.99304464 1.02656216]\n",
      " [0.62915744 0.745617   0.1880254  ... 0.74694568 0.45843666 0.60019522]\n",
      " [0.18539095 0.40095796 0.56763318 ... 0.71285492 0.93666004 0.72865204]\n",
      " ...\n",
      " [0.78054244 0.82034091 0.08668523 ... 0.12371276 0.03258136 0.32830463]\n",
      " [0.27987872 0.96589011 0.37420643 ... 0.57979563 0.32915841 0.83956452]\n",
      " [0.82904337 0.55661354 0.86177935 ... 0.68258725 0.69042625 0.77770358]].\n",
      "Wrong output values for b1 vector.\n",
      "\t Expected: [[0.27581438]\n",
      " [0.33807755]\n",
      " [0.59087163]\n",
      " [0.45787607]\n",
      " [0.31688845]\n",
      " [0.5186393 ]\n",
      " [0.00596877]\n",
      " [0.7707525 ]\n",
      " [0.20032616]\n",
      " [0.15449703]] \n",
      "\tGot: [[ 0.6279002 ]\n",
      " [ 0.13092078]\n",
      " [ 0.04552323]\n",
      " [-0.03916117]\n",
      " [ 0.8325815 ]\n",
      " [ 0.72378233]\n",
      " [ 0.38506879]\n",
      " [ 0.08189845]\n",
      " [ 0.53399369]\n",
      " [ 0.62530561]].\n",
      "Wrong output values for gradient of b2 vector.\n",
      "\t Expected: [[0.85118393]\n",
      " [0.0772051 ]\n",
      " [0.77574519]\n",
      " ...\n",
      " [0.26061528]\n",
      " [0.67110752]\n",
      " [0.83251038]] \n",
      "\tGot: [[0.64755019]\n",
      " [0.85850704]\n",
      " [0.78235059]\n",
      " ...\n",
      " [0.36202761]\n",
      " [0.18994367]\n",
      " [0.52721988]].\n",
      "name small_check\n",
      "iters: 10 cost: 7.774701\n",
      "Wrong output shape for W1 matrix.\n",
      "\t Expected: (5, 5778) \n",
      "\tGot: (5, 5775).\n",
      "Wrong output shape for W2 matrix.\n",
      "\t Expected: (5778, 5) \n",
      "\tGot: (5775, 5).\n",
      "Wrong output shape for b2 vector.\n",
      "\t Expected: (5778, 1) \n",
      "\tGot: (5775, 1).\n",
      "Wrong output values for W1 matrix.\n",
      "\t Expected: [[0.21955121 0.87004448 0.20671916 ... 0.07752275 0.77457191 0.67174422]\n",
      " [0.27577893 0.532172   0.30534023 ... 0.38543118 0.28671259 0.13483498]\n",
      " [0.987288   0.46881859 0.28648156 ... 0.8206767  0.03792429 0.60534254]\n",
      " [0.34221277 0.59118683 0.27437007 ... 0.24937364 0.37304776 0.02650941]\n",
      " [0.86029217 0.86354    0.9106285  ... 0.30000818 0.2322273  0.7837576 ]] \n",
      "\tGot: [[0.21852853 0.8692063  0.20671916 ... 0.94856911 0.43857341 0.76175763]\n",
      " [0.07228757 0.7706973  0.67174422 ... 0.66479061 0.12487353 0.78392502]\n",
      " [0.90476319 0.28276752 0.90278781 ... 0.78956051 0.39281928 0.42028964]\n",
      " [0.58152805 0.43041766 0.28586166 ... 0.36018539 0.35759206 0.54584511]\n",
      " [0.96694236 0.89642635 0.89907326 ... 0.21473647 0.09405947 0.95583177]].\n",
      "Wrong output values for W2 matrix.\n",
      "\t Expected: [[0.76320106 0.54504084 0.93378556 0.73599188 1.01550456]\n",
      " [0.12166387 1.00536763 0.93126515 0.05322734 0.83143197]\n",
      " [0.10183734 0.04003451 0.99304244 0.70575225 0.27900097]\n",
      " ...\n",
      " [0.44888249 0.68064226 0.25140446 0.82963476 0.9178071 ]\n",
      " [0.02947705 0.82557035 0.36649501 0.4966427  0.4188333 ]\n",
      " [0.64371739 0.51447761 0.43217087 0.32061607 0.31237681]] \n",
      "\tGot: [[0.23534933 0.44061855 0.63354448 0.70314275 0.73152675]\n",
      " [0.93421365 0.1199763  1.01004624 0.61503994 0.5704206 ]\n",
      " [0.49261531 0.16878108 0.29998427 0.23221722 0.78373806]\n",
      " ...\n",
      " [0.92256748 0.11709173 0.40440236 0.39238508 0.79189848]\n",
      " [0.72018994 0.97810695 0.64524461 0.20964472 0.78705389]\n",
      " [0.02895223 0.70038558 0.68308869 0.57766008 0.89048879]].\n",
      "Wrong output values for b1 vector.\n",
      "\t Expected: [[0.25442659]\n",
      " [0.66697987]\n",
      " [0.28046412]\n",
      " [0.01315516]\n",
      " [0.28675058]] \n",
      "\tGot: [[0.38632968]\n",
      " [0.61602159]\n",
      " [0.79186628]\n",
      " [0.01612798]\n",
      " [0.62738999]].\n",
      "Wrong output values for gradient of b2 vector.\n",
      "\t Expected: [[0.18226354]\n",
      " [0.46436747]\n",
      " [0.61834337]\n",
      " ...\n",
      " [0.75831577]\n",
      " [0.6020428 ]\n",
      " [0.90920931]] \n",
      "\tGot: [[0.86942006]\n",
      " [0.07340987]\n",
      " [0.81009339]\n",
      " ...\n",
      " [0.11864383]\n",
      " [0.65426484]\n",
      " [0.7139223 ]].\n",
      "\u001b[92m 2  Tests passed\n",
      "\u001b[91m 14  Tests failed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_gradient_descent(gradient_descent, data, word2Ind, N=10, V=len(word2Ind), num_iters=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b455e7",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3.0 Visualizing the word vectors\n",
    "\n",
    "In this part you will visualize the word vectors trained using the function you just coded above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01461a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50) [2744, 3949, 2960, 3022, 5672, 1452, 5671, 4189, 2315, 4276]\n"
     ]
    }
   ],
   "source": [
    "# # visualizing the word vectors here\n",
    "# from matplotlib import pyplot\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "# words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
    "#          'rich','happy','sad']\n",
    "\n",
    "# embs = (W1.T + W2)/2.0\n",
    " \n",
    "# # given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "# idx = [word2Ind[word] for word in words]\n",
    "# X = embs[idx, :]\n",
    "# print(X.shape, idx)  # X.shape:  Number of words of dimension N each \n",
    "\n",
    "# Visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "words = ['king', 'queen', 'lord', 'man', 'woman', 'dog', 'wolf', 'rich', 'happy', 'sad']\n",
    "\n",
    "# print('W1.shape: ',W1.shape,'\\t','W2.shape: ', W2.shape)\n",
    "embs = (W1.T + W2) / 2.0\n",
    "# print(\"embs.shape\", embs.shape)\n",
    "\n",
    "# Given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx,:]\n",
    "print(X.shape, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec02a7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()\n",
    "'''\n",
    "\n",
    "result = compute_pca(X, 5)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])`\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 4]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f90d64",
   "metadata": {},
   "source": [
    "You can see that man and king are next to each other. However, we have to be careful with the interpretation of this projected word vectors, since the PCA depends on the projection -- as shown in the following illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a442b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
